{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2WBYP81X0px9XIAYdjR/J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vpilone/CMPSC497-Midterm/blob/main/CMPSC497_Midterm_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UANCgVj34Zn_",
        "outputId": "bb87cff1-51ac-4425-84db-06853dab5f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install datasets\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "trainDataset = load_dataset(\"tomaarsen/MultiCoNER\", \"en\", split=\"train\")\n",
        "# trainDataset = trainDataset.with_format(\"tf\")\n",
        "testDataset = load_dataset(\"tomaarsen/MultiCoNER\", \"en\", split=\"test\")\n",
        "# testDataset = testDataset.with_format(\"tf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9hwGHVr4h2N",
        "outputId": "c5947ccc-591b-48dd-9116-d015c213351d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabToIndexDict = {}\n",
        "indexToVocabDict = {}\n",
        "curIndex = 1\n",
        "\n",
        "trainingSentances = []\n",
        "vocabWords = []\n",
        "\n",
        "for sentance in trainDataset[:]['tokens']:\n",
        "  trainingSentances.append(sentance)\n",
        "  for word in sentance:\n",
        "    if word not in vocabWords:\n",
        "      vocabWords.append(word)\n",
        "    if vocabToIndexDict.get(word) == None:\n",
        "      vocabToIndexDict[word] = curIndex\n",
        "      indexToVocabDict[curIndex] = word\n",
        "      curIndex +=1"
      ],
      "metadata": {
        "id": "BnGmI2qs5y_3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu4OrsSB_myM",
        "outputId": "4c87252f-0a7e-4e74-dbbe-f1b9b675d1fc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "w2vModel = Word2Vec(trainDataset[:]['tokens'], min_count=1, window=4, vector_size=50,negative=3)\n",
        "w2vModel.add_null_word()"
      ],
      "metadata": {
        "id": "v2aiDL2NDL8y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w2vModel.wv.distance('his','he'))\n",
        "print(w2vModel.wv.distance('queen','his'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRmPpqYKHC9O",
        "outputId": "2b4f16ec-4096-4cf3-fbbf-71ec5b85d339"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.020865559577941895\n",
            "0.04645603895187378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unknownVocab = []\n",
        "sentanceEmbedded = []\n",
        "sentanceEmbedded.append(tf.stack([[0]*50]))\n",
        "totalEmbbeded = []\n",
        "batch = 0\n",
        "for sentance in trainDataset[:]['tokens']:\n",
        "  batch += 1\n",
        "  wordEmbedded = []\n",
        "  for word in sentance:\n",
        "    try:\n",
        "      # wordEmbedded.append(w2vModel.wv[word])\n",
        "      wordEmbedded.append(w2vModel[word])\n",
        "    except:\n",
        "      unknownVocab.append(word)\n",
        "      # wordEmbedded.append([1]*w2vModel.wv.vector_size)\n",
        "      wordEmbedded.append([1]*w2vModel.vector_size)\n",
        "  sentanceEmbedded.append(tf.stack(wordEmbedded))\n",
        "paddedEmbedded = utils.pad_sequences(sentanceEmbedded, padding=\"post\")\n",
        "totalEmbbeded=(tf.stack(paddedEmbedded))\n",
        "#print(sentanceEmbedded)"
      ],
      "metadata": {
        "id": "yUcMgDqUKpYX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#version for Sparse Categorical Error\n",
        "newEncodedData = []\n",
        "newEncodedData.append(tf.stack([[0]*50]))\n",
        "\n",
        "for newSentance in trainDataset[:]['tokens']:\n",
        "  newEncodedSentance = []\n",
        "  for newWord in newSentance:\n",
        "    try:\n",
        "      #newEncodedSentance.append(w2vModel.wv[word])\n",
        "      newEncodedSentance.append(w2vModel[newWord])\n",
        "    except:\n",
        "      #newEncodedSentance.append([0]*w2vModel.wv.vector_size)\n",
        "      newEncodedSentance.append([1]*w2vModel.vector_size)\n",
        "  newEncodedData.append(tf.stack(newEncodedSentance))\n",
        "newPaddedData = utils.pad_sequences(newEncodedData, padding=\"post\", dtype=float)\n",
        "\n",
        "print(newPaddedData.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKty6_rm9Po0",
        "outputId": "653c90aa-00ba-4acb-b5d1-76ac9362f4eb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(15301, 41, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newTags = []\n",
        "newTags.append([0]*50)\n",
        "\n",
        "for newTag in trainDataset[:]['ner_tags']:\n",
        "  newTags.append(newTag)\n",
        "newPaddedTags = utils.pad_sequences(newTags, padding=\"post\")\n",
        "newStackedPaddedTags = tf.stack(newPaddedTags)"
      ],
      "metadata": {
        "id": "B7bu3s-0dv-K"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import losses as losses\n",
        "from tensorflow.keras import metrics as mts\n",
        "\n",
        "newRNNModel = models.Sequential()\n",
        "newRNNModel.add(layers.SimpleRNN(units=50))\n",
        "newRNNModel.add(layers.Dense(units=50))\n",
        "# newRNNModel.add(layers.)\n",
        "newRNNModel.compile(loss=losses.CosineSimilarity(), metrics=['accuracy'])\n",
        "#newRNNModel.compile(loss=losses.cosine_similarity, metrics=[mts.Accuracy])\n",
        "newRNNModel.fit(newPaddedData, newPaddedTags, epochs=5)\n",
        "# # print(type(w2vModel.wv['test']))\n",
        "# # print(inputs)\n",
        "# # print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXWP0n9teATR",
        "outputId": "81dd50a4-340d-434d-eafc-93639d772e73"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 9ms/step - accuracy: 0.0807 - loss: -0.3542\n",
            "Epoch 2/5\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.1233 - loss: -0.4358\n",
            "Epoch 3/5\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.1523 - loss: -0.4486\n",
            "Epoch 4/5\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.1618 - loss: -0.4500\n",
            "Epoch 5/5\n",
            "\u001b[1m479/479\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.1746 - loss: -0.4591\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aba8549f950>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#version for Sparse Categorical Error\n",
        "newEncodedTestData = []\n",
        "newEncodedTestData.append(tf.stack([[0]*50]))\n",
        "\n",
        "for newTestSentance in testDataset[:2000]['tokens']:\n",
        "  newEncodedTestSentance = []\n",
        "  for newTestWord in newTestSentance:\n",
        "    try:\n",
        "      newEncodedTestSentance.append(w2vModel.wv[newTestWord])\n",
        "      #newTestSentance.append(w2vModel[newTestWord])\n",
        "    except:\n",
        "      #newEncodedSentance.append([1]*w2vModel.wv.vector_size)\n",
        "      newEncodedTestSentance.append([1]*w2vModel.vector_size)\n",
        "  newEncodedTestData.append(tf.stack(newEncodedTestSentance))\n",
        "newPaddedTestData = utils.pad_sequences(newEncodedTestData, padding=\"post\", dtype=float)\n",
        "\n",
        "print(newPaddedTestData.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCpnuZBBL9h9",
        "outputId": "e4f08afe-2717-45be-9087-480c6f7bf96b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2001, 36, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newTestTags = []\n",
        "newTestTags.append([0]*50)\n",
        "\n",
        "for newTestTag in testDataset[:2000]['ner_tags']:\n",
        "  newTestTags.append(newTestTag)\n",
        "newPaddedTestTags = utils.pad_sequences(newTestTags, padding=\"post\")\n",
        "newStackedPaddedTestTags = tf.stack(newPaddedTestTags)\n",
        "\n",
        "print(newStackedPaddedTestTags.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRcg_EIL61K",
        "outputId": "02ee7334-5285-442d-e0ba-b1f5149f57a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2001, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = newRNNModel.evaluate(newPaddedTestData, newPaddedTestTags)\n",
        "print(\"Loss: \" ,results[0])\n",
        "print(\"Accuracy: \" ,results[1]*100, \"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8xhArO1LS8D",
        "outputId": "49239dd9-e65c-41df-f92b-c6c7a7e483d2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 11ms/step - accuracy: 0.0686 - loss: -0.3917\n",
            "Loss:  -0.3908470571041107\n",
            "Accuracy:  7.446277141571045 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "17yrNX3WS3Gd"
      }
    }
  ]
}